{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-25T19:13:45.853695Z",
     "start_time": "2020-02-25T19:13:44.992515Z"
    }
   },
   "outputs": [],
   "source": [
    "import io\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torchtext\n",
    "from torchtext.datasets import text_classification\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torchtext.utils import download_from_url, extract_archive, unicode_csv_reader\n",
    "from torchtext.data.utils import ngrams_iterator\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "from torchtext.vocab import Vocab\n",
    "from tqdm import tqdm\n",
    "\n",
    "pd.set_option('display.max_colwidth', -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Saving data from torchtext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-22T20:32:25.371658Z",
     "start_time": "2020-02-22T20:32:25.368378Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "NGRAMS = 2\n",
    "DATADIR = \"./data\"\n",
    "BATCH_SIZE = 16\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-22T20:32:55.383167Z",
     "start_time": "2020-02-22T20:32:25.840295Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "120000lines [00:07, 15473.49lines/s]\n",
      "120000lines [00:14, 8531.16lines/s]\n",
      "7600lines [00:00, 8824.13lines/s]\n"
     ]
    }
   ],
   "source": [
    "if not os.path.isdir(DATADIR):\n",
    "    os.mkdir(DATADIR)\n",
    "train_dataset, test_dataset = text_classification.DATASETS['AG_NEWS'](\n",
    "    root=DATADIR, ngrams=NGRAMS, vocab=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Define the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-22T20:24:40.932045Z",
     "start_time": "2020-02-22T20:24:40.925685Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "class TextSentiment(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, num_class):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.EmbeddingBag(vocab_size, embed_dim, sparse=True)\n",
    "        self.fc = nn.Linear(embed_dim, num_class)\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        initrange = 0.5\n",
    "        self.embedding.weight.data.uniform_(-initrange, initrange)\n",
    "        self.fc.weight.data.uniform_(-initrange, initrange)\n",
    "        self.fc.bias.data.zero_()\n",
    "\n",
    "    def forward(self, text, offsets):\n",
    "        embedded = self.embedding(text, offsets)\n",
    "        return self.fc(embedded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create torchdataset from csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-25T19:13:48.137526Z",
     "start_time": "2020-02-25T19:13:48.121958Z"
    }
   },
   "outputs": [],
   "source": [
    "def _csv_iterator(data_path, ngrams, yield_cls=False):\n",
    "    tokenizer = get_tokenizer(\"basic_english\")\n",
    "    with io.open(data_path, encoding=\"utf8\") as f:\n",
    "        reader = unicode_csv_reader(f)\n",
    "        for row in reader:\n",
    "            tokens = ' '.join(row[1:])\n",
    "            tokens = tokenizer(tokens)\n",
    "            if yield_cls:\n",
    "                yield int(row[0]) - 1, ngrams_iterator(tokens, ngrams)\n",
    "            else:\n",
    "                yield ngrams_iterator(tokens, ngrams)\n",
    "\n",
    "def _create_data_from_iterator(vocab, iterator, include_unk):\n",
    "    data = []\n",
    "    labels = []\n",
    "    with tqdm(unit_scale=0, unit='lines') as t:\n",
    "        for cls, tokens in iterator:\n",
    "            if include_unk:\n",
    "                tokens = torch.tensor([vocab[token] for token in tokens])\n",
    "            else:\n",
    "                token_ids = list(filter(lambda x: x is not Vocab.UNK, [vocab[token]\n",
    "                                        for token in tokens]))\n",
    "                tokens = torch.tensor(token_ids)\n",
    "            if len(tokens) == 0:\n",
    "                logging.info('Row contains no tokens.')\n",
    "            data.append((cls, tokens))\n",
    "            labels.append(cls)\n",
    "            t.update(1)\n",
    "    return data, set(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-25T19:13:48.696689Z",
     "start_time": "2020-02-25T19:13:48.693630Z"
    }
   },
   "outputs": [],
   "source": [
    "train_csv_path = \"data/ag_news_csv/train.csv\"\n",
    "test_csv_path = \"data/ag_news_csv/test.csv\"\n",
    "ngrams = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-22T21:06:06.290430Z",
     "start_time": "2020-02-22T21:05:55.005762Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "120000lines [00:07, 15170.35lines/s]\n"
     ]
    }
   ],
   "source": [
    "vocab = build_vocab_from_iterator(_csv_iterator(train_csv_path, ngrams))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-22T21:06:06.311646Z",
     "start_time": "2020-02-22T21:06:06.303718Z"
    }
   },
   "outputs": [],
   "source": [
    "train_iterator = _csv_iterator(train_csv_path, ngrams, yield_cls=True)\n",
    "test_iterator = _csv_iterator(train_csv_path, ngrams, yield_cls=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-22T21:06:21.645599Z",
     "start_time": "2020-02-22T21:06:06.937696Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "120000lines [00:14, 8162.34lines/s]\n"
     ]
    }
   ],
   "source": [
    "train_data_set, labels = _create_data_from_iterator(vocab, train_iterator, include_unk=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initiate an instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions used to generate batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define functions to train the model and evaluate results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
