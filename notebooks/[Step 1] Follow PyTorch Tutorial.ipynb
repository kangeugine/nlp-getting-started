{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-25T22:10:54.250691Z",
     "start_time": "2020-02-25T22:10:53.722310Z"
    }
   },
   "outputs": [],
   "source": [
    "import io\n",
    "import os\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "import torchtext\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchtext.datasets import text_classification, TextClassificationDataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data.dataset import random_split\n",
    "from torchtext.utils import download_from_url, extract_archive, unicode_csv_reader\n",
    "from torchtext.data.utils import ngrams_iterator\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "from torchtext.vocab import Vocab\n",
    "\n",
    "pd.set_option('display.max_colwidth', -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving data from torchtext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-25T22:11:00.417011Z",
     "start_time": "2020-02-25T22:11:00.413608Z"
    }
   },
   "outputs": [],
   "source": [
    "NGRAMS = 2\n",
    "DATADIR = \"./data\"\n",
    "BATCH_SIZE = 16\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-25T19:21:54.621208Z",
     "start_time": "2020-02-25T19:21:21.892448Z"
    }
   },
   "outputs": [],
   "source": [
    "if not os.path.isdir(DATADIR):\n",
    "    os.mkdir(DATADIR)\n",
    "train_dataset, test_dataset = text_classification.DATASETS['AG_NEWS'](\n",
    "    root=DATADIR, ngrams=NGRAMS, vocab=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-25T22:11:03.213219Z",
     "start_time": "2020-02-25T22:11:03.207211Z"
    }
   },
   "outputs": [],
   "source": [
    "class TextSentiment(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, num_class):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.EmbeddingBag(vocab_size, embed_dim, sparse=True)\n",
    "        self.fc = nn.Linear(embed_dim, num_class)\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        initrange = 0.5\n",
    "        self.embedding.weight.data.uniform_(-initrange, initrange)\n",
    "        self.fc.weight.data.uniform_(-initrange, initrange)\n",
    "        self.fc.bias.data.zero_()\n",
    "\n",
    "    def forward(self, text, offsets):\n",
    "        embedded = self.embedding(text, offsets)\n",
    "        return self.fc(embedded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create torchdataset from csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-25T22:11:05.057485Z",
     "start_time": "2020-02-25T22:11:05.047876Z"
    }
   },
   "outputs": [],
   "source": [
    "def _csv_iterator(data_path, ngrams, yield_cls=False):\n",
    "    tokenizer = get_tokenizer(\"basic_english\")\n",
    "    with io.open(data_path, encoding=\"utf8\") as f:\n",
    "        reader = unicode_csv_reader(f)\n",
    "        for row in reader:\n",
    "            tokens = ' '.join(row[1:])\n",
    "            tokens = tokenizer(tokens)\n",
    "            if yield_cls:\n",
    "                yield int(row[0]) - 1, ngrams_iterator(tokens, ngrams)\n",
    "            else:\n",
    "                yield ngrams_iterator(tokens, ngrams)\n",
    "\n",
    "def _create_data_from_iterator(vocab, iterator, include_unk):\n",
    "    data = []\n",
    "    labels = []\n",
    "    with tqdm(unit_scale=0, unit='lines') as t:\n",
    "        for cls, tokens in iterator:\n",
    "            if include_unk:\n",
    "                tokens = torch.tensor([vocab[token] for token in tokens])\n",
    "            else:\n",
    "                token_ids = list(filter(lambda x: x is not Vocab.UNK, [vocab[token]\n",
    "                                        for token in tokens]))\n",
    "                tokens = torch.tensor(token_ids)\n",
    "            if len(tokens) == 0:\n",
    "                logging.info('Row contains no tokens.')\n",
    "            data.append((cls, tokens))\n",
    "            labels.append(cls)\n",
    "            t.update(1)\n",
    "    return data, set(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-25T22:18:24.132488Z",
     "start_time": "2020-02-25T22:18:24.129730Z"
    }
   },
   "outputs": [],
   "source": [
    "train_csv_path = \"../data/ag_news_csv/train.csv\"\n",
    "test_csv_path = \"../data/ag_news_csv/test.csv\"\n",
    "ngrams = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-25T22:18:35.735200Z",
     "start_time": "2020-02-25T22:18:24.387808Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "120000lines [00:07, 15070.03lines/s]\n"
     ]
    }
   ],
   "source": [
    "vocab = build_vocab_from_iterator(_csv_iterator(train_csv_path, ngrams))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-25T22:18:35.741474Z",
     "start_time": "2020-02-25T22:18:35.738125Z"
    }
   },
   "outputs": [],
   "source": [
    "train_iterator = _csv_iterator(train_csv_path, ngrams, yield_cls=True)\n",
    "test_iterator = _csv_iterator(train_csv_path, ngrams, yield_cls=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-25T22:19:04.119826Z",
     "start_time": "2020-02-25T22:18:35.743199Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "120000lines [00:14, 8464.71lines/s]\n",
      "120000lines [00:14, 8526.46lines/s]\n"
     ]
    }
   ],
   "source": [
    "train_data_set, labels = _create_data_from_iterator(vocab, train_iterator, include_unk=False)\n",
    "test_data_set, test_labels = _create_data_from_iterator(vocab, test_iterator, include_unk=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initiate an instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-25T22:11:33.886617Z",
     "start_time": "2020-02-25T22:11:33.300810Z"
    }
   },
   "outputs": [],
   "source": [
    "VOCAB_SIZE = len(vocab)\n",
    "EMBED_DIM = 32\n",
    "NUM_CLASS = len(labels)\n",
    "model = TextSentiment(VOCAB_SIZE, EMBED_DIM, NUM_CLASS).to(DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions used to generate batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-25T22:11:33.892491Z",
     "start_time": "2020-02-25T22:11:33.888420Z"
    }
   },
   "outputs": [],
   "source": [
    "def generate_batch(batch):\n",
    "    label = torch.tensor([entry[0] for entry in batch])\n",
    "    text = [entry[1] for entry in batch]\n",
    "    offsets = [0] + [len(entry) for entry in text]\n",
    "    offsets = torch.tensor(offsets[:-1]).cumsum(dim=0)\n",
    "    text = torch.cat(text)\n",
    "    return text, offsets, label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define functions to train the model and evaluate results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-25T22:11:33.902931Z",
     "start_time": "2020-02-25T22:11:33.894291Z"
    }
   },
   "outputs": [],
   "source": [
    "def train_func(sub_train_):\n",
    "    # Train the model\n",
    "    train_loss = 0\n",
    "    train_acc = 0\n",
    "    data = DataLoader(sub_train_, batch_size=BATCH_SIZE, shuffle=True, collate_fn=generate_batch)\n",
    "    for i, (text, offsets, cls) in enumerate(data):\n",
    "        optimizer.zero_grad()\n",
    "        text, offsets, cls = text.to(DEVICE), offsets.to(DEVICE), cls.to(DEVICE)\n",
    "        output = model(text, offsets)\n",
    "        loss = criterion(output, cls)\n",
    "        train_loss += loss.item()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_acc += (output.argmax(1) == cls).sum().item()\n",
    "        \n",
    "    # Adjust the learning rate\n",
    "    scheduler.step()\n",
    "    \n",
    "    return train_loss / len(sub_train_), train_acc / len(sub_train_)\n",
    "\n",
    "def test(data_):\n",
    "    loss = 0\n",
    "    acc = 0\n",
    "    data = DataLoader(data_, batch_size=BATCH_SIZE, collate_fn=generate_batch)\n",
    "    for text, offsets, cls in data:\n",
    "        text, offsets, cls = text.to(DEVICE), offsets.to(DEVICE), cls.to(DEVICE)\n",
    "        with torch.no_grad():\n",
    "            output = model(text, offsets)\n",
    "            loss = criterion(output, cls)\n",
    "            loss += loss.item()\n",
    "            acc += (output.argmax(1) == cls).sum().item()\n",
    "    \n",
    "    return loss / len(data_), acc / len(data_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split the dataset and run the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-25T22:11:33.906621Z",
     "start_time": "2020-02-25T22:11:33.904380Z"
    }
   },
   "outputs": [],
   "source": [
    "N_EPOCHS = 5\n",
    "min_valid_loss = float('inf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-25T22:14:19.031201Z",
     "start_time": "2020-02-25T22:12:17.200220Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 time in 0 minutes, 24 seconds\n",
      "\tLoss: 0.0123(train)\t|\tAcc: 93.4%(train)\n",
      "\tLoss: 0.0000(valid)\t|\tAcc: 92.1%(valid)\n",
      "Epoch: 2 time in 0 minutes, 24 seconds\n",
      "\tLoss: 0.0070(train)\t|\tAcc: 96.3%(train)\n",
      "\tLoss: 0.0001(valid)\t|\tAcc: 92.7%(valid)\n",
      "Epoch: 3 time in 0 minutes, 24 seconds\n",
      "\tLoss: 0.0038(train)\t|\tAcc: 98.1%(train)\n",
      "\tLoss: 0.0001(valid)\t|\tAcc: 93.3%(valid)\n",
      "Epoch: 4 time in 0 minutes, 24 seconds\n",
      "\tLoss: 0.0022(train)\t|\tAcc: 99.0%(train)\n",
      "\tLoss: 0.0000(valid)\t|\tAcc: 93.8%(valid)\n",
      "Epoch: 5 time in 0 minutes, 25 seconds\n",
      "\tLoss: 0.0015(train)\t|\tAcc: 99.4%(train)\n",
      "\tLoss: 0.0000(valid)\t|\tAcc: 93.7%(valid)\n"
     ]
    }
   ],
   "source": [
    "criterion = torch.nn.CrossEntropyLoss().to(DEVICE)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=4.0)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1, gamma=0.9)\n",
    "\n",
    "train_len = int(len(train_data_set) * 0.95)\n",
    "sub_train_, sub_valid_ = random_split(train_data_set, [train_len, len(train_data_set) - train_len])\n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "    start_time = time.time()\n",
    "    train_loss, train_acc = train_func(sub_train_)\n",
    "    valid_loss, valid_acc = test(sub_valid_)\n",
    "    \n",
    "    secs = int(time.time() - start_time)\n",
    "    mins = secs / 60\n",
    "    secs = secs % 60\n",
    "    \n",
    "    print('Epoch: %d' %(epoch + 1), \"time in %d minutes, %d seconds\" %(mins, secs))\n",
    "    print(f'\\tLoss: {train_loss:.4f}(train)\\t|\\tAcc: {train_acc * 100:.1f}%(train)')\n",
    "    print(f'\\tLoss: {valid_loss:.4f}(valid)\\t|\\tAcc: {valid_acc * 100:.1f}%(valid)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate the model with test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-25T22:19:17.698421Z",
     "start_time": "2020-02-25T22:19:15.813065Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLoss: 0.0000(test)\t|\tAcc: 99.4%(test)\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_acc = test(test_data_set)\n",
    "print(f'\\tLoss: {test_loss:.4f}(test)\\t|\\tAcc: {test_acc * 100:.1f}%(test)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test on a random news"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-25T22:20:59.981367Z",
     "start_time": "2020-02-25T22:20:59.978610Z"
    }
   },
   "outputs": [],
   "source": [
    "ag_news_label = {1 : \"World\",\n",
    "                 2 : \"Sports\",\n",
    "                 3 : \"Business\",\n",
    "                 4 : \"Sci/Tec\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-25T22:22:12.894886Z",
     "start_time": "2020-02-25T22:22:12.890630Z"
    }
   },
   "outputs": [],
   "source": [
    "def predict(text, model, vocab, ngrams):\n",
    "    tokenizer = get_tokenizer(\"basic_english\")\n",
    "    with torch.no_grad():\n",
    "        text = torch.tensor([vocab[token] for token in ngrams_iterator(tokenizer(text), ngrams)])\n",
    "        output = model(text, torch.tensor([0]))\n",
    "        return output.argmax(1).item() + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-25T22:22:14.756159Z",
     "start_time": "2020-02-25T22:22:14.753388Z"
    }
   },
   "outputs": [],
   "source": [
    "ex_text_str = \"MEMPHIS, Tenn. – Four days ago, Jon Rahm was \\\n",
    "    enduring the season’s worst weather conditions on Sunday at The \\\n",
    "    Open on his way to a closing 75 at Royal Portrush, which \\\n",
    "    considering the wind and the rain was a respectable showing. \\\n",
    "    Thursday’s first round at the WGC-FedEx St. Jude Invitational \\\n",
    "    was another story. With temperatures in the mid-80s and hardly any \\\n",
    "    wind, the Spaniard was 13 strokes better in a flawless round. \\\n",
    "    Thanks to his best putting performance on the PGA Tour, Rahm \\\n",
    "    finished with an 8-under 62 for a three-stroke lead, which \\\n",
    "    was even more impressive considering he’d never played the \\\n",
    "    front nine at TPC Southwind.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-25T22:22:49.207295Z",
     "start_time": "2020-02-25T22:22:49.194910Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is a Sports news\n"
     ]
    }
   ],
   "source": [
    "print(\"This is a %s news\" %ag_news_label[predict(ex_text_str, model, vocab, 2)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
