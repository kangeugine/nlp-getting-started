{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Follow tutorial for nlp-getting-started"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-26T18:44:07.744522Z",
     "start_time": "2020-02-26T18:44:07.155788Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import io\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import math\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "import torch\n",
    "import torchtext\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchtext.datasets import text_classification, TextClassificationDataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data.dataset import random_split\n",
    "from torchtext.utils import download_from_url, extract_archive, unicode_csv_reader\n",
    "from torchtext.data.utils import ngrams_iterator\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "from torchtext.vocab import Vocab\n",
    "\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "from src.clean_text import clean_text\n",
    "pd.set_option('display.max_colwidth', -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create torchdataset from csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-26T18:44:07.756134Z",
     "start_time": "2020-02-26T18:44:07.746983Z"
    }
   },
   "outputs": [],
   "source": [
    "def _csv_iterator(data_path, ngrams, yield_cls=False):\n",
    "    tokenizer = get_tokenizer(\"basic_english\")\n",
    "    with io.open(data_path, encoding=\"utf8\") as f:\n",
    "        reader = unicode_csv_reader(f)\n",
    "        next(reader)\n",
    "        for row in reader:\n",
    "            tokens = ' '.join(row[1:4])\n",
    "            tokens = clean_text(tokens)\n",
    "            tokens = tokenizer(tokens)\n",
    "            if yield_cls:\n",
    "                yield int(row[4]), ngrams_iterator(tokens, ngrams)\n",
    "            else:\n",
    "                yield ngrams_iterator(tokens, ngrams)\n",
    "\n",
    "def _create_data_from_iterator(vocab, iterator, include_unk):\n",
    "    data = []\n",
    "    labels = []\n",
    "    with tqdm(unit_scale=0, unit='lines') as t:\n",
    "        for cls, tokens in iterator:\n",
    "            if include_unk:\n",
    "                tokens = torch.tensor([vocab[token] for token in tokens])\n",
    "            else:\n",
    "                token_ids = list(filter(lambda x: x is not Vocab.UNK, [vocab[token]\n",
    "                                        for token in tokens]))\n",
    "                tokens = torch.tensor(token_ids)\n",
    "            if len(tokens) == 0:\n",
    "                logging.info('Row contains no tokens.')\n",
    "            data.append((cls, tokens))\n",
    "            labels.append(cls)\n",
    "            t.update(1)\n",
    "    return data, set(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-26T18:44:07.764069Z",
     "start_time": "2020-02-26T18:44:07.757604Z"
    }
   },
   "outputs": [],
   "source": [
    "train_data_path = \"../data/train.csv\"\n",
    "test_data_path = \"../data/test.csv\"\n",
    "NGRAMS = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-26T18:44:08.746471Z",
     "start_time": "2020-02-26T18:44:07.766588Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7613lines [00:00, 24804.58lines/s]\n",
      "7613lines [00:00, 14892.23lines/s]\n"
     ]
    }
   ],
   "source": [
    "vocab = build_vocab_from_iterator(_csv_iterator(train_data_path, NGRAMS))\n",
    "train_iterator = _csv_iterator(train_data_path, NGRAMS, yield_cls=True)\n",
    "test_iterator = _csv_iterator(test_data_path, NGRAMS, yield_cls=False)\n",
    "train_data_set, labels = _create_data_from_iterator(vocab, train_iterator, include_unk=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Define and initiate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-26T18:44:08.753506Z",
     "start_time": "2020-02-26T18:44:08.748173Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "class TextSentiment(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, num_class):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.EmbeddingBag(vocab_size, embed_dim, sparse=True)\n",
    "        self.fc = nn.Linear(embed_dim, num_class)\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        initrange = 0.5\n",
    "        self.embedding.weight.data.uniform_(-initrange, initrange)\n",
    "        self.fc.weight.data.uniform_(-initrange, initrange)\n",
    "        self.fc.bias.data.zero_()\n",
    "\n",
    "    def forward(self, text, offsets):\n",
    "        embedded = self.embedding(text, offsets)\n",
    "        return self.fc(embedded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-26T18:44:08.797753Z",
     "start_time": "2020-02-26T18:44:08.754846Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "VOCAB_SIZE = len(vocab)\n",
    "EMBED_DIM = 32\n",
    "NUM_CLASS = len(labels)\n",
    "BATCH_SIZE = 16\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = TextSentiment(VOCAB_SIZE, EMBED_DIM, NUM_CLASS).to(DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Functions used to generate batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-26T18:44:08.803287Z",
     "start_time": "2020-02-26T18:44:08.799505Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def generate_batch(batch):\n",
    "    label = torch.tensor([entry[0] for entry in batch])\n",
    "    text = [entry[1] for entry in batch]\n",
    "    offsets = [0] + [len(entry) for entry in text]\n",
    "    offsets = torch.tensor(offsets[:-1]).cumsum(dim=0)\n",
    "    text = torch.cat(text)\n",
    "    return text, offsets, label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-26T17:47:16.847949Z",
     "start_time": "2020-02-26T17:47:16.842898Z"
    }
   },
   "source": [
    "## Define functions to train the model and evaluate results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-26T18:44:08.814267Z",
     "start_time": "2020-02-26T18:44:08.804961Z"
    }
   },
   "outputs": [],
   "source": [
    "def train_func(sub_train_):\n",
    "    # Train the model\n",
    "    train_loss = 0\n",
    "    y_true = np.array([])\n",
    "    y_pred = np.array([])\n",
    "    data = DataLoader(sub_train_, batch_size=BATCH_SIZE, shuffle=True, collate_fn=generate_batch)\n",
    "    for i, (text, offsets, cls) in enumerate(data):\n",
    "        optimizer.zero_grad()\n",
    "        text, offsets, cls = text.to(DEVICE), offsets.to(DEVICE), cls.to(DEVICE)\n",
    "        output = model(text, offsets)\n",
    "        loss = criterion(output, cls)\n",
    "        train_loss += loss.item()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        y_true = np.append(y_true, cls.numpy())\n",
    "        y_pred = np.append(y_pred, output.argmax(1).numpy())\n",
    "        \n",
    "    # Adjust the learning rate\n",
    "    scheduler.step()\n",
    "    \n",
    "    return train_loss / len(sub_train_), f1_score(y_true, y_pred)\n",
    "\n",
    "def test(data_):\n",
    "    loss = 0\n",
    "    y_true = np.array([])\n",
    "    y_pred = np.array([])\n",
    "    data = DataLoader(data_, batch_size=BATCH_SIZE, collate_fn=generate_batch)\n",
    "    for text, offsets, cls in data:\n",
    "        text, offsets, cls = text.to(DEVICE), offsets.to(DEVICE), cls.to(DEVICE)\n",
    "        with torch.no_grad():\n",
    "            output = model(text, offsets)\n",
    "            loss = criterion(output, cls)\n",
    "            loss += loss.item()\n",
    "            y_true = np.append(y_true, cls.numpy())\n",
    "            y_pred = np.append(y_pred, output.argmax(1).numpy())\n",
    "    \n",
    "    return loss / len(data_), f1_score(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Split the dataset and run the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-26T18:44:08.818170Z",
     "start_time": "2020-02-26T18:44:08.815781Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "N_EPOCHS = 50\n",
    "min_valid_loss = float('inf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-26T18:44:47.783985Z",
     "start_time": "2020-02-26T18:44:08.819849Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 time in 0 minutes, 0 seconds\n",
      "\tLoss: 0.0420(train)\t|\tF1: 56.9%(train)\n",
      "\tLoss: 0.0011(valid)\t|\tF1: 72.2%(valid)\n",
      "Epoch: 2 time in 0 minutes, 0 seconds\n",
      "\tLoss: 0.0257(train)\t|\tF1: 78.3%(train)\n",
      "\tLoss: 0.0042(valid)\t|\tF1: 66.2%(valid)\n",
      "Epoch: 3 time in 0 minutes, 0 seconds\n",
      "\tLoss: 0.0164(train)\t|\tF1: 87.9%(train)\n",
      "\tLoss: 0.0011(valid)\t|\tF1: 74.6%(valid)\n",
      "Epoch: 4 time in 0 minutes, 0 seconds\n",
      "\tLoss: 0.0100(train)\t|\tF1: 94.3%(train)\n",
      "\tLoss: 0.0002(valid)\t|\tF1: 64.7%(valid)\n",
      "Epoch: 5 time in 0 minutes, 0 seconds\n",
      "\tLoss: 0.0065(train)\t|\tF1: 96.7%(train)\n",
      "\tLoss: 0.0016(valid)\t|\tF1: 74.9%(valid)\n",
      "Epoch: 6 time in 0 minutes, 0 seconds\n",
      "\tLoss: 0.0049(train)\t|\tF1: 97.6%(train)\n",
      "\tLoss: 0.0015(valid)\t|\tF1: 75.9%(valid)\n",
      "Epoch: 7 time in 0 minutes, 0 seconds\n",
      "\tLoss: 0.0041(train)\t|\tF1: 98.0%(train)\n",
      "\tLoss: 0.0008(valid)\t|\tF1: 75.2%(valid)\n",
      "Epoch: 8 time in 0 minutes, 0 seconds\n",
      "\tLoss: 0.0038(train)\t|\tF1: 98.4%(train)\n",
      "\tLoss: 0.0010(valid)\t|\tF1: 75.0%(valid)\n",
      "Epoch: 9 time in 0 minutes, 0 seconds\n",
      "\tLoss: 0.0035(train)\t|\tF1: 98.6%(train)\n",
      "\tLoss: 0.0015(valid)\t|\tF1: 75.3%(valid)\n",
      "Epoch: 10 time in 0 minutes, 0 seconds\n",
      "\tLoss: 0.0027(train)\t|\tF1: 98.6%(train)\n",
      "\tLoss: 0.0010(valid)\t|\tF1: 75.7%(valid)\n",
      "Epoch: 11 time in 0 minutes, 0 seconds\n",
      "\tLoss: 0.0030(train)\t|\tF1: 98.7%(train)\n",
      "\tLoss: 0.0012(valid)\t|\tF1: 75.8%(valid)\n",
      "Epoch: 12 time in 0 minutes, 0 seconds\n",
      "\tLoss: 0.0026(train)\t|\tF1: 98.9%(train)\n",
      "\tLoss: 0.0010(valid)\t|\tF1: 75.6%(valid)\n",
      "Epoch: 13 time in 0 minutes, 0 seconds\n",
      "\tLoss: 0.0024(train)\t|\tF1: 98.9%(train)\n",
      "\tLoss: 0.0009(valid)\t|\tF1: 76.3%(valid)\n",
      "Epoch: 14 time in 0 minutes, 0 seconds\n",
      "\tLoss: 0.0022(train)\t|\tF1: 98.9%(train)\n",
      "\tLoss: 0.0011(valid)\t|\tF1: 76.3%(valid)\n",
      "Epoch: 15 time in 0 minutes, 0 seconds\n",
      "\tLoss: 0.0022(train)\t|\tF1: 98.9%(train)\n",
      "\tLoss: 0.0011(valid)\t|\tF1: 76.0%(valid)\n",
      "Epoch: 16 time in 0 minutes, 0 seconds\n",
      "\tLoss: 0.0021(train)\t|\tF1: 99.0%(train)\n",
      "\tLoss: 0.0010(valid)\t|\tF1: 76.7%(valid)\n",
      "Epoch: 17 time in 0 minutes, 0 seconds\n",
      "\tLoss: 0.0019(train)\t|\tF1: 99.0%(train)\n",
      "\tLoss: 0.0010(valid)\t|\tF1: 76.2%(valid)\n",
      "Epoch: 18 time in 0 minutes, 0 seconds\n",
      "\tLoss: 0.0016(train)\t|\tF1: 99.1%(train)\n",
      "\tLoss: 0.0012(valid)\t|\tF1: 75.4%(valid)\n",
      "Epoch: 19 time in 0 minutes, 0 seconds\n",
      "\tLoss: 0.0015(train)\t|\tF1: 99.0%(train)\n",
      "\tLoss: 0.0011(valid)\t|\tF1: 75.9%(valid)\n",
      "Epoch: 20 time in 0 minutes, 0 seconds\n",
      "\tLoss: 0.0015(train)\t|\tF1: 99.1%(train)\n",
      "\tLoss: 0.0011(valid)\t|\tF1: 75.6%(valid)\n",
      "Epoch: 21 time in 0 minutes, 0 seconds\n",
      "\tLoss: 0.0014(train)\t|\tF1: 99.1%(train)\n",
      "\tLoss: 0.0011(valid)\t|\tF1: 75.9%(valid)\n",
      "Epoch: 22 time in 0 minutes, 0 seconds\n",
      "\tLoss: 0.0014(train)\t|\tF1: 99.0%(train)\n",
      "\tLoss: 0.0011(valid)\t|\tF1: 76.3%(valid)\n",
      "Epoch: 23 time in 0 minutes, 0 seconds\n",
      "\tLoss: 0.0014(train)\t|\tF1: 99.2%(train)\n",
      "\tLoss: 0.0011(valid)\t|\tF1: 75.9%(valid)\n",
      "Epoch: 24 time in 0 minutes, 0 seconds\n",
      "\tLoss: 0.0013(train)\t|\tF1: 99.2%(train)\n",
      "\tLoss: 0.0012(valid)\t|\tF1: 75.8%(valid)\n",
      "Epoch: 25 time in 0 minutes, 0 seconds\n",
      "\tLoss: 0.0013(train)\t|\tF1: 99.2%(train)\n",
      "\tLoss: 0.0011(valid)\t|\tF1: 76.2%(valid)\n",
      "Epoch: 26 time in 0 minutes, 0 seconds\n",
      "\tLoss: 0.0013(train)\t|\tF1: 99.3%(train)\n",
      "\tLoss: 0.0011(valid)\t|\tF1: 76.1%(valid)\n",
      "Epoch: 27 time in 0 minutes, 0 seconds\n",
      "\tLoss: 0.0012(train)\t|\tF1: 99.2%(train)\n",
      "\tLoss: 0.0011(valid)\t|\tF1: 76.6%(valid)\n",
      "Epoch: 28 time in 0 minutes, 0 seconds\n",
      "\tLoss: 0.0012(train)\t|\tF1: 99.2%(train)\n",
      "\tLoss: 0.0012(valid)\t|\tF1: 75.6%(valid)\n",
      "Epoch: 29 time in 0 minutes, 0 seconds\n",
      "\tLoss: 0.0012(train)\t|\tF1: 99.2%(train)\n",
      "\tLoss: 0.0011(valid)\t|\tF1: 75.8%(valid)\n",
      "Epoch: 30 time in 0 minutes, 0 seconds\n",
      "\tLoss: 0.0011(train)\t|\tF1: 99.2%(train)\n",
      "\tLoss: 0.0011(valid)\t|\tF1: 75.9%(valid)\n",
      "Epoch: 31 time in 0 minutes, 0 seconds\n",
      "\tLoss: 0.0011(train)\t|\tF1: 99.2%(train)\n",
      "\tLoss: 0.0011(valid)\t|\tF1: 76.0%(valid)\n",
      "Epoch: 32 time in 0 minutes, 0 seconds\n",
      "\tLoss: 0.0010(train)\t|\tF1: 99.2%(train)\n",
      "\tLoss: 0.0011(valid)\t|\tF1: 76.5%(valid)\n",
      "Epoch: 33 time in 0 minutes, 0 seconds\n",
      "\tLoss: 0.0010(train)\t|\tF1: 99.3%(train)\n",
      "\tLoss: 0.0011(valid)\t|\tF1: 76.3%(valid)\n",
      "Epoch: 34 time in 0 minutes, 0 seconds\n",
      "\tLoss: 0.0010(train)\t|\tF1: 99.2%(train)\n",
      "\tLoss: 0.0011(valid)\t|\tF1: 76.3%(valid)\n",
      "Epoch: 35 time in 0 minutes, 0 seconds\n",
      "\tLoss: 0.0010(train)\t|\tF1: 99.2%(train)\n",
      "\tLoss: 0.0011(valid)\t|\tF1: 76.3%(valid)\n",
      "Epoch: 36 time in 0 minutes, 0 seconds\n",
      "\tLoss: 0.0010(train)\t|\tF1: 99.3%(train)\n",
      "\tLoss: 0.0011(valid)\t|\tF1: 76.7%(valid)\n",
      "Epoch: 37 time in 0 minutes, 0 seconds\n",
      "\tLoss: 0.0010(train)\t|\tF1: 99.2%(train)\n",
      "\tLoss: 0.0011(valid)\t|\tF1: 76.8%(valid)\n",
      "Epoch: 38 time in 0 minutes, 0 seconds\n",
      "\tLoss: 0.0010(train)\t|\tF1: 99.2%(train)\n",
      "\tLoss: 0.0011(valid)\t|\tF1: 76.0%(valid)\n",
      "Epoch: 39 time in 0 minutes, 0 seconds\n",
      "\tLoss: 0.0010(train)\t|\tF1: 99.2%(train)\n",
      "\tLoss: 0.0011(valid)\t|\tF1: 76.6%(valid)\n",
      "Epoch: 40 time in 0 minutes, 0 seconds\n",
      "\tLoss: 0.0010(train)\t|\tF1: 99.2%(train)\n",
      "\tLoss: 0.0011(valid)\t|\tF1: 76.5%(valid)\n",
      "Epoch: 41 time in 0 minutes, 0 seconds\n",
      "\tLoss: 0.0010(train)\t|\tF1: 99.2%(train)\n",
      "\tLoss: 0.0011(valid)\t|\tF1: 76.2%(valid)\n",
      "Epoch: 42 time in 0 minutes, 0 seconds\n",
      "\tLoss: 0.0010(train)\t|\tF1: 99.2%(train)\n",
      "\tLoss: 0.0011(valid)\t|\tF1: 76.0%(valid)\n",
      "Epoch: 43 time in 0 minutes, 0 seconds\n",
      "\tLoss: 0.0009(train)\t|\tF1: 99.2%(train)\n",
      "\tLoss: 0.0011(valid)\t|\tF1: 76.5%(valid)\n",
      "Epoch: 44 time in 0 minutes, 0 seconds\n",
      "\tLoss: 0.0009(train)\t|\tF1: 99.2%(train)\n",
      "\tLoss: 0.0011(valid)\t|\tF1: 76.5%(valid)\n",
      "Epoch: 45 time in 0 minutes, 0 seconds\n",
      "\tLoss: 0.0009(train)\t|\tF1: 99.3%(train)\n",
      "\tLoss: 0.0011(valid)\t|\tF1: 76.2%(valid)\n",
      "Epoch: 46 time in 0 minutes, 0 seconds\n",
      "\tLoss: 0.0009(train)\t|\tF1: 99.2%(train)\n",
      "\tLoss: 0.0011(valid)\t|\tF1: 76.1%(valid)\n",
      "Epoch: 47 time in 0 minutes, 0 seconds\n",
      "\tLoss: 0.0009(train)\t|\tF1: 99.2%(train)\n",
      "\tLoss: 0.0011(valid)\t|\tF1: 76.1%(valid)\n",
      "Epoch: 48 time in 0 minutes, 0 seconds\n",
      "\tLoss: 0.0009(train)\t|\tF1: 99.2%(train)\n",
      "\tLoss: 0.0011(valid)\t|\tF1: 76.2%(valid)\n",
      "Epoch: 49 time in 0 minutes, 0 seconds\n",
      "\tLoss: 0.0009(train)\t|\tF1: 99.2%(train)\n",
      "\tLoss: 0.0011(valid)\t|\tF1: 76.2%(valid)\n",
      "Epoch: 50 time in 0 minutes, 0 seconds\n",
      "\tLoss: 0.0009(train)\t|\tF1: 99.2%(train)\n",
      "\tLoss: 0.0011(valid)\t|\tF1: 76.1%(valid)\n"
     ]
    }
   ],
   "source": [
    "criterion = torch.nn.CrossEntropyLoss().to(DEVICE)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=4.0)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1, gamma=0.9)\n",
    "\n",
    "train_len = int(len(train_data_set) * 0.90)\n",
    "sub_train_, sub_valid_ = random_split(train_data_set, [train_len, len(train_data_set) - train_len])\n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "    start_time = time.time()\n",
    "    train_loss, train_f1 = train_func(sub_train_)\n",
    "    valid_loss, valid_f1 = test(sub_valid_)\n",
    "    \n",
    "    secs = int(time.time() - start_time)\n",
    "    mins = secs / 60\n",
    "    secs = secs % 60\n",
    "    \n",
    "    print('Epoch: %d' %(epoch + 1), \"time in %d minutes, %d seconds\" %(mins, secs))\n",
    "    print(f'\\tLoss: {train_loss:.4f}(train)\\t|\\tF1: {train_f1 * 100:.1f}%(train)')\n",
    "    print(f'\\tLoss: {valid_loss:.4f}(valid)\\t|\\tF1: {valid_f1 * 100:.1f}%(valid)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train model on entire dataset\n",
    " - expected F1 score ~ 76.1%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-26T18:47:42.732762Z",
     "start_time": "2020-02-26T18:47:00.401295Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 time in 0 minutes, 0 seconds\n",
      "\tLoss: 0.0417(train)\t|\tF1: 57.6%(train)\n",
      "Epoch: 2 time in 0 minutes, 0 seconds\n",
      "\tLoss: 0.0254(train)\t|\tF1: 78.5%(train)\n",
      "Epoch: 3 time in 0 minutes, 0 seconds\n",
      "\tLoss: 0.0162(train)\t|\tF1: 88.0%(train)\n",
      "Epoch: 4 time in 0 minutes, 0 seconds\n",
      "\tLoss: 0.0095(train)\t|\tF1: 94.2%(train)\n",
      "Epoch: 5 time in 0 minutes, 0 seconds\n",
      "\tLoss: 0.0066(train)\t|\tF1: 96.7%(train)\n",
      "Epoch: 6 time in 0 minutes, 0 seconds\n",
      "\tLoss: 0.0056(train)\t|\tF1: 97.4%(train)\n",
      "Epoch: 7 time in 0 minutes, 0 seconds\n",
      "\tLoss: 0.0042(train)\t|\tF1: 98.1%(train)\n",
      "Epoch: 8 time in 0 minutes, 0 seconds\n",
      "\tLoss: 0.0040(train)\t|\tF1: 98.2%(train)\n",
      "Epoch: 9 time in 0 minutes, 0 seconds\n",
      "\tLoss: 0.0040(train)\t|\tF1: 98.3%(train)\n",
      "Epoch: 10 time in 0 minutes, 0 seconds\n",
      "\tLoss: 0.0033(train)\t|\tF1: 98.4%(train)\n",
      "Epoch: 11 time in 0 minutes, 0 seconds\n",
      "\tLoss: 0.0031(train)\t|\tF1: 98.5%(train)\n",
      "Epoch: 12 time in 0 minutes, 0 seconds\n",
      "\tLoss: 0.0028(train)\t|\tF1: 98.6%(train)\n",
      "Epoch: 13 time in 0 minutes, 0 seconds\n",
      "\tLoss: 0.0026(train)\t|\tF1: 98.7%(train)\n",
      "Epoch: 14 time in 0 minutes, 0 seconds\n",
      "\tLoss: 0.0021(train)\t|\tF1: 98.9%(train)\n",
      "Epoch: 15 time in 0 minutes, 0 seconds\n",
      "\tLoss: 0.0023(train)\t|\tF1: 98.7%(train)\n",
      "Epoch: 16 time in 0 minutes, 0 seconds\n",
      "\tLoss: 0.0021(train)\t|\tF1: 98.8%(train)\n",
      "Epoch: 17 time in 0 minutes, 0 seconds\n",
      "\tLoss: 0.0018(train)\t|\tF1: 98.8%(train)\n",
      "Epoch: 18 time in 0 minutes, 0 seconds\n",
      "\tLoss: 0.0019(train)\t|\tF1: 99.0%(train)\n",
      "Epoch: 19 time in 0 minutes, 0 seconds\n",
      "\tLoss: 0.0019(train)\t|\tF1: 98.9%(train)\n",
      "Epoch: 20 time in 0 minutes, 0 seconds\n",
      "\tLoss: 0.0017(train)\t|\tF1: 99.0%(train)\n",
      "Epoch: 21 time in 0 minutes, 0 seconds\n",
      "\tLoss: 0.0017(train)\t|\tF1: 98.9%(train)\n",
      "Epoch: 22 time in 0 minutes, 0 seconds\n",
      "\tLoss: 0.0015(train)\t|\tF1: 99.1%(train)\n",
      "Epoch: 23 time in 0 minutes, 0 seconds\n",
      "\tLoss: 0.0015(train)\t|\tF1: 99.1%(train)\n",
      "Epoch: 24 time in 0 minutes, 0 seconds\n",
      "\tLoss: 0.0014(train)\t|\tF1: 99.1%(train)\n",
      "Epoch: 25 time in 0 minutes, 0 seconds\n",
      "\tLoss: 0.0014(train)\t|\tF1: 99.2%(train)\n",
      "Epoch: 26 time in 0 minutes, 0 seconds\n",
      "\tLoss: 0.0014(train)\t|\tF1: 99.1%(train)\n",
      "Epoch: 27 time in 0 minutes, 0 seconds\n",
      "\tLoss: 0.0013(train)\t|\tF1: 99.1%(train)\n",
      "Epoch: 28 time in 0 minutes, 0 seconds\n",
      "\tLoss: 0.0013(train)\t|\tF1: 99.0%(train)\n",
      "Epoch: 29 time in 0 minutes, 0 seconds\n",
      "\tLoss: 0.0013(train)\t|\tF1: 99.2%(train)\n",
      "Epoch: 30 time in 0 minutes, 0 seconds\n",
      "\tLoss: 0.0012(train)\t|\tF1: 99.1%(train)\n",
      "Epoch: 31 time in 0 minutes, 0 seconds\n",
      "\tLoss: 0.0012(train)\t|\tF1: 99.1%(train)\n",
      "Epoch: 32 time in 0 minutes, 0 seconds\n",
      "\tLoss: 0.0012(train)\t|\tF1: 99.2%(train)\n",
      "Epoch: 33 time in 0 minutes, 0 seconds\n",
      "\tLoss: 0.0012(train)\t|\tF1: 99.2%(train)\n",
      "Epoch: 34 time in 0 minutes, 0 seconds\n",
      "\tLoss: 0.0011(train)\t|\tF1: 99.1%(train)\n",
      "Epoch: 35 time in 0 minutes, 0 seconds\n",
      "\tLoss: 0.0011(train)\t|\tF1: 99.1%(train)\n",
      "Epoch: 36 time in 0 minutes, 0 seconds\n",
      "\tLoss: 0.0011(train)\t|\tF1: 99.1%(train)\n",
      "Epoch: 37 time in 0 minutes, 0 seconds\n",
      "\tLoss: 0.0011(train)\t|\tF1: 99.1%(train)\n",
      "Epoch: 38 time in 0 minutes, 0 seconds\n",
      "\tLoss: 0.0011(train)\t|\tF1: 99.1%(train)\n",
      "Epoch: 39 time in 0 minutes, 0 seconds\n",
      "\tLoss: 0.0011(train)\t|\tF1: 99.1%(train)\n",
      "Epoch: 40 time in 0 minutes, 0 seconds\n",
      "\tLoss: 0.0011(train)\t|\tF1: 99.1%(train)\n",
      "Epoch: 41 time in 0 minutes, 0 seconds\n",
      "\tLoss: 0.0011(train)\t|\tF1: 99.1%(train)\n",
      "Epoch: 42 time in 0 minutes, 0 seconds\n",
      "\tLoss: 0.0011(train)\t|\tF1: 99.2%(train)\n",
      "Epoch: 43 time in 0 minutes, 0 seconds\n",
      "\tLoss: 0.0011(train)\t|\tF1: 99.2%(train)\n",
      "Epoch: 44 time in 0 minutes, 0 seconds\n",
      "\tLoss: 0.0010(train)\t|\tF1: 99.2%(train)\n",
      "Epoch: 45 time in 0 minutes, 0 seconds\n",
      "\tLoss: 0.0010(train)\t|\tF1: 99.2%(train)\n",
      "Epoch: 46 time in 0 minutes, 0 seconds\n",
      "\tLoss: 0.0010(train)\t|\tF1: 99.1%(train)\n",
      "Epoch: 47 time in 0 minutes, 0 seconds\n",
      "\tLoss: 0.0010(train)\t|\tF1: 99.1%(train)\n",
      "Epoch: 48 time in 0 minutes, 0 seconds\n",
      "\tLoss: 0.0010(train)\t|\tF1: 99.1%(train)\n",
      "Epoch: 49 time in 0 minutes, 0 seconds\n",
      "\tLoss: 0.0010(train)\t|\tF1: 99.2%(train)\n",
      "Epoch: 50 time in 0 minutes, 0 seconds\n",
      "\tLoss: 0.0010(train)\t|\tF1: 99.2%(train)\n"
     ]
    }
   ],
   "source": [
    "model = TextSentiment(VOCAB_SIZE, EMBED_DIM, NUM_CLASS).to(DEVICE)\n",
    "criterion = torch.nn.CrossEntropyLoss().to(DEVICE)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=4.0)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1, gamma=0.9)\n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "    start_time = time.time()\n",
    "    train_loss, train_f1 = train_func(train_data_set)\n",
    "    secs = int(time.time() - start_time)\n",
    "    mins = secs / 60\n",
    "    secs = secs % 60\n",
    "    \n",
    "    print('Epoch: %d' %(epoch + 1), \"time in %d minutes, %d seconds\" %(mins, secs))\n",
    "    print(f'\\tLoss: {train_loss:.4f}(train)\\t|\\tF1: {train_f1 * 100:.1f}%(train)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate the model with test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-26T19:07:12.688470Z",
     "start_time": "2020-02-26T19:07:12.683426Z"
    }
   },
   "outputs": [],
   "source": [
    "def _create_test_data_from_iterator(vocab, iterator, include_unk):\n",
    "    data = []\n",
    "    with tqdm(unit_scale=0, unit='lines') as t:\n",
    "        for tokens in iterator:\n",
    "            if include_unk:\n",
    "                tokens = torch.tensor([vocab[token] for token in tokens])\n",
    "            else:\n",
    "                token_ids = list(filter(lambda x: x is not Vocab.UNK, [vocab[token]\n",
    "                                        for token in tokens]))\n",
    "                tokens = torch.tensor(token_ids)\n",
    "            if len(tokens) == 0:\n",
    "                logging.info('Row contains no tokens.')\n",
    "            data.append(tokens)\n",
    "            t.update(1)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-26T19:07:43.501948Z",
     "start_time": "2020-02-26T19:07:43.282139Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3263lines [00:00, 15151.72lines/s]\n"
     ]
    }
   ],
   "source": [
    "test_data_set = _create_test_data_from_iterator(vocab, test_iterator, include_unk=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-26T19:09:52.049252Z",
     "start_time": "2020-02-26T19:09:52.045899Z"
    }
   },
   "outputs": [],
   "source": [
    "def predict(text, model):\n",
    "    with torch.no_grad():\n",
    "        output = model(text, torch.tensor([0]))\n",
    "        return output.argmax(1).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-26T19:10:09.191538Z",
     "start_time": "2020-02-26T19:10:08.883601Z"
    }
   },
   "outputs": [],
   "source": [
    "test_y_pred = [predict(i, model) for i in test_data_set]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-26T19:15:00.008705Z",
     "start_time": "2020-02-26T19:14:59.978859Z"
    }
   },
   "outputs": [],
   "source": [
    "test = pd.read_csv(\"../data/test.csv\")\n",
    "test[\"target\"] = test_y_pred\n",
    "test_submit = test[[\"id\", \"target\"]]\n",
    "test_submit.to_csv(\"../builds/submission/baseline.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-26T19:16:29.751449Z",
     "start_time": "2020-02-26T19:16:29.747786Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3263, 5)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
